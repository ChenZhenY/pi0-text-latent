{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import copy\n",
    "import os.path\n",
    "import pickle\n",
    "from os.path import join\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import ImageDraw\n",
    "from matplotlib import cm\n",
    "\n",
    "script_path = \"/home/shady/code/openpi/scripts\"\n",
    "os.environ[\"PYTHONPATH\"] += f\":{script_path}\"\n",
    "import jax.experimental\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "\n",
    "from openpi import EXP_DATA_PATH\n",
    "from openpi.models.pi0 import Pi0\n",
    "\n",
    "script_path = \"/home/shady/code/openpi/scripts\"\n",
    "os.environ[\"PYTHONPATH\"] += f\":{script_path}\"\n",
    "from scripts.text_latent import Checkpoint, Args, _model, _config, maybe_download, create_dataloader\n",
    "\n",
    "\n",
    "def restore_img(img) -> Image:\n",
    "    float_img = np.asarray(img + 1, dtype=np.float64) / 2 * 255\n",
    "    int_img = float_img.astype(np.uint8)\n",
    "    img = Image.fromarray(int_img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def normalize(vectors):\n",
    "    return vectors / jnp.linalg.norm(vectors, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def patch_idx_to_hw(patch_idx, num_patch=16, patch_size=14):\n",
    "    \"\"\"Convert patch index to height and width.\"\"\"\n",
    "    h = patch_idx // num_patch\n",
    "    w = patch_idx % num_patch\n",
    "    return int(h * patch_size), int(w * patch_size)\n",
    "\n",
    "\n",
    "def draw_box_on_image(image, top_left_corner, box_width=14, box_height=14, color='red', thickness=1):\n",
    "    # Create a Draw object to modify the image\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Extract coordinates and calculate the bounding box\n",
    "    top = top_left_corner[0]\n",
    "    left = top_left_corner[1]\n",
    "    right = left + box_width\n",
    "    bottom = top + box_height\n",
    "\n",
    "    bounding_box = (left, top, right, bottom)\n",
    "\n",
    "    # Draw the rectangle\n",
    "    draw.rectangle(bounding_box, outline=color, width=thickness)\n",
    "\n",
    "    return image\n"
   ],
   "id": "56b204e773b48f54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load text latents for libero goal task\n",
    "text_latent_path = join(EXP_DATA_PATH, \"pi0\")\n",
    "libero_goal_task_id = [i for i in range(10, 40)]\n",
    "text_latents = {}\n",
    "for id in libero_goal_task_id:\n",
    "    file_path = join(text_latent_path, f\"avg_states_{id}_{id + 1}_frame_0_119.pkl\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    text_latents[id] = data[\"hidden_states_avg\"]\n",
    "# load training data for libero goal task\n",
    "policy = \"pi0\"\n",
    "args = Args()\n",
    "args.policy = Checkpoint(config=f\"{policy}_libero\", dir=\"s3://openpi-assets/checkpoints/{}_libero\".format(policy))\n",
    "train_config = _config.get_config(args.policy.config)\n",
    "data_config = train_config.data.create(train_config.assets_dirs, train_config.model)\n",
    "ckpt_dir = maybe_download(args.policy.dir)\n",
    "\n",
    "# load model\n",
    "model: Pi0 = train_config.model.load(_model.restore_params(ckpt_dir / \"params\", dtype=jnp.bfloat16))\n",
    "embed_prefix_jit = jax.jit(model.embed_prefix)\n",
    "\n",
    "\n",
    "def embed_prefix(observation):\n",
    "    observation = _model.preprocess_observation(None, observation, train=False)\n",
    "    return embed_prefix_jit(observation)\n"
   ],
   "id": "ecb37027b698e269",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_obs = []\n",
    "for task_id in range(10, 40):\n",
    "    task_range = (task_id, task_id + 1)\n",
    "    episode_to_use_for_collection = 1\n",
    "    data_loader, dataset_meta = create_dataloader(train_config, data_config, ckpt_dir, task_range, 1,\n",
    "                                                  episode_to_use_for_collection, True)\n",
    "    obs = [_model.Observation.from_dict(element) for element in list(iter(data_loader))]\n",
    "    base_images = [obs[i].images[\"base_0_rgb\"][0] for i in range(len(obs))]\n",
    "    print(f\"Task {task_id}: {dataset_meta.tasks[task_id]}\")\n",
    "\n",
    "\n",
    "    def decode(obs):\n",
    "        if not isinstance(obs, list):\n",
    "            obs = obs.tokenized_prompt[0].tolist()\n",
    "        return data_loader.torch_loader.dataset._transform.transforms[-1].tokenizer._tokenizer.decode(obs)\n",
    "\n",
    "\n",
    "    prompt = obs[0].tokenized_prompt[0]\n",
    "    decoded = [decode([token.item()]) for token in prompt]\n",
    "    prompt = decoded[:obs[0].tokenized_prompt_mask.sum()]\n",
    "    print(f\"Tokenized prompt: {prompt}\")\n",
    "    print(f\"Token index: {prompt[:obs[0].tokenized_prompt_mask.sum()]}\")\n",
    "\n",
    "    # check embedding same\n",
    "    prompt_len = len(decoded[:obs[0].tokenized_prompt_mask.sum()])\n",
    "    all_obs.append((obs, len(prompt), prompt, len(obs)))\n"
   ],
   "id": "d132058c4470ce9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test\n",
    "def get_max_sim_patch(task_start, task_id, all_obs, obs_idx=0):\n",
    "    task_obs = copy.deepcopy(all_obs[task_id - task_start])\n",
    "    obs = task_obs[0]\n",
    "    prompt_len = task_obs[1]\n",
    "    episode_similarity = jnp.zeros((18, 256), dtype=jnp.float32)\n",
    "\n",
    "    # slightly average to remove noise\n",
    "    # for obs_idx in tqdm.tqdm(range(70, 71)):\n",
    "    embeddings, _, _ = embed_prefix(obs[obs_idx])  # test embedding prefix\n",
    "\n",
    "    image_embedding = embeddings[0, :256]\n",
    "    text_latent = text_latents[task_id][:, 256 * 3 + 3:256 * 3 + prompt_len].sum(-2)\n",
    "    normed_image_embedding = normalize(image_embedding)\n",
    "    normed_text_latent = normalize(text_latent)\n",
    "\n",
    "    # embeddings, _, _ = embed_prefix(obs[obs_idx])  # test embedding prefix\n",
    "    # text_embedding = embeddings[0, prompt_len - 2]\n",
    "    # image_latent = text_latents[task_id][:, :256]\n",
    "    # normed_image_embedding = normalize(image_latent)\n",
    "    # normed_text_latent = normalize(text_embedding)\n",
    "\n",
    "    similarity = jnp.dot(normed_text_latent, normed_image_embedding.T)\n",
    "    episode_similarity += similarity\n",
    "\n",
    "    patch_episode_similarity = jnp.max(episode_similarity[1:], axis=0)\n",
    "    # max_patch_indices = jnp.argsort(patch_episode_similarity)[-35:]\n",
    "    # original_img = restore_img(copy.deepcopy(base_images[0]))\n",
    "    # for max_patch_index in max_patch_indices:\n",
    "    #     top_left_coordinate = patch_idx_to_hw(max_patch_index)\n",
    "    #     draw_box_on_image(original_img, top_left_coordinate)\n",
    "\n",
    "    float_img = np.asarray(task_obs[0][obs_idx].images[\"base_0_rgb\"][0] + 1, dtype=np.float64) / 2 * 255\n",
    "    original_image = float_img.astype(np.uint8)\n",
    "    coorrelate_map = np.asarray(patch_episode_similarity).reshape(16, 16)\n",
    "    norm_attention_map = (coorrelate_map - coorrelate_map.min()) / (coorrelate_map.max() - coorrelate_map.min())\n",
    "    resized_attention_map = cv2.resize(norm_attention_map, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    heatmap_rgba = cm.jet(resized_attention_map)\n",
    "    heatmap_bgr = (heatmap_rgba[:, :, :3] * 255).astype(np.uint8)\n",
    "    alpha = 0.3\n",
    "    blended_image = cv2.addWeighted(heatmap_bgr, alpha, original_image, 1 - alpha, 0)\n",
    "    return Image.fromarray(blended_image)\n",
    "    #\n",
    "    # return patch_episode_similarity, base_images\n",
    "\n",
    "\n",
    "def plot_dynamics(task_id):\n",
    "    task_obs = all_obs[task_id - 10]\n",
    "    print(f\"Task {task_id}: {task_obs[2]}\")\n",
    "    images = [get_max_sim_patch(10, task_id, all_obs, obs_idx=t) for t in range(0, task_obs[-1], 5)]\n",
    "\n",
    "    def show_image(index):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(images[index])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(images) - 1, step=1, description='Image:')\n",
    "    widgets.interact(show_image, index=slider)"
   ],
   "id": "b8b1ee8c3e30cad2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_dynamics(22)",
   "id": "83d00ac508f74f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_dynamics(29)",
   "id": "511f26d7d7f788e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_dynamics(23)",
   "id": "c070adb31132cf49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_dynamics(27)",
   "id": "61a6f17fcbd00184",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
